{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像尺寸对训练效果的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from utils import get_params_count\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.applications import inception_v3, xception, resnet50, vgg16, vgg19\n",
    "from keras.applications import InceptionV3, Xception, ResNet50, VGG16, VGG19\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_input(height, width, input_type):\n",
    "    train_name = os.listdir('./train')\n",
    "    test_name = os.listdir('./test')\n",
    "    train_size = len(train_name)\n",
    "    test_size = len(test_name)\n",
    "    X_train = np.zeros((train_size, height, width, 3), dtype=input_type)\n",
    "    X_test = np.zeros((test_size, height, width, 3), dtype=input_type)\n",
    "    \n",
    "    labels = pd.read_csv('labels.csv')\n",
    "    breeds = list(set(labels['breed']))\n",
    "    breeds.sort()\n",
    "    Y_test = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "    # Labels\n",
    "    Y_train = np.zeros((train_size, len(breeds)), dtype=np.uint8)\n",
    "    for i in range(train_size):\n",
    "        onehot = breeds.index(labels['breed'][i])\n",
    "        Y_train[i][onehot] = 1\n",
    "\n",
    "    # Train data\n",
    "    for i in tqdm(range(train_size)):\n",
    "        img = cv2.imread('./train/%s.jpg' % labels['id'][i])\n",
    "        img = cv2.resize(img, dsize=(width, height))\n",
    "        img = img[:, :, ::-1]\n",
    "        X_train[i] = img\n",
    "\n",
    "    # Test data\n",
    "    for i in tqdm(range(test_size)):\n",
    "        img = cv2.imread('./test/%s.jpg' % Y_test['id'][i])\n",
    "        img = cv2.resize(img, dsize=(width, height))\n",
    "        img = img[:, :, ::-1]\n",
    "        X_test[i] = img\n",
    "\n",
    "    print(\"Train: %d, Test: %d\" % (train_size, test_size))\n",
    "    print(\"Total Dog Breeds:\", len(breeds))\n",
    "    print('Training Data Size = %.2f GB' % (sys.getsizeof(X_train)/1024**3))\n",
    "    print('Testing Data Size = %.2f GB' % (sys.getsizeof(X_test)/1024**3))\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 299\n",
    "width = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 10222/10222 [00:27<00:00, 376.18it/s]\n",
      "100%|█████████████████████████████████████████████████████| 10357/10357 [00:26<00:00, 384.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10222, Test: 10357\n",
      "Total Dog Breeds: 120\n",
      "Training Data Size = 2.55 GB\n",
      "Testing Data Size = 2.59 GB\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_input(height, width, np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 120)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_gap(MODEL, height, width, train, test, preprocess=None, batch_size=128):\n",
    "    x = Input(shape=(height, width, 3))\n",
    "    if preprocess is not None:\n",
    "        x = Lambda(preprocess)(x)\n",
    "    model = MODEL(include_top=False, input_tensor=x, weights='imagenet', pooling='avg')\n",
    "    train_gap = model.predict(train, batch_size=batch_size)\n",
    "    test_gap = model.predict(test, batch_size=batch_size)\n",
    "    with h5py.File(\"gap_%dx%d_%s.h5\" % (height, width, MODEL.__name__), 'w') as f:\n",
    "        f.create_dataset('train', data=train_gap)\n",
    "        f.create_dataset('test', data=test_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_gap(InceptionV3, height, width, X_train, X_test, inception_v3.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 2048\n"
     ]
    }
   ],
   "source": [
    "train_gap = []\n",
    "test_gap = []\n",
    "# 'gap_InceptionV3.h5', 'gap_Xception.h5', 'gap_ResNet50.h5', 'gap_InceptionResNetV2.h5'\n",
    "for gapfile in ['gap_299x299_InceptionV3.h5']:\n",
    "    with h5py.File(gapfile, 'r') as f:\n",
    "        train_gap.append(np.array(f['train']))\n",
    "        test_gap.append(np.array(f['test']))\n",
    "train_gap = np.concatenate(train_gap, axis=1)\n",
    "test_gap = np.concatenate(test_gap, axis=1)\n",
    "print(\"Number of Features:\", train_gap.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_gap, X_val_gap, Y_train, Y_val = train_test_split(train_gap, Y_train, shuffle=True, test_size=0.2, \n",
    "                                                          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 245880, Non-Trainable: 0\n",
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/20\n",
      "8177/8177 [==============================] - 1s 138us/step - loss: 2.2564 - acc: 0.6661 - val_loss: 0.9128 - val_acc: 0.8782\n",
      "Epoch 2/20\n",
      "8177/8177 [==============================] - 1s 111us/step - loss: 0.6235 - acc: 0.8940 - val_loss: 0.4943 - val_acc: 0.8910\n",
      "Epoch 3/20\n",
      "8177/8177 [==============================] - 1s 111us/step - loss: 0.3961 - acc: 0.9128 - val_loss: 0.3842 - val_acc: 0.9042\n",
      "Epoch 4/20\n",
      "8177/8177 [==============================] - 1s 113us/step - loss: 0.3151 - acc: 0.9215 - val_loss: 0.3414 - val_acc: 0.9007\n",
      "Epoch 5/20\n",
      "8177/8177 [==============================] - 1s 116us/step - loss: 0.2766 - acc: 0.9233 - val_loss: 0.3158 - val_acc: 0.9037\n",
      "Epoch 6/20\n",
      "8177/8177 [==============================] - 1s 119us/step - loss: 0.2520 - acc: 0.9282 - val_loss: 0.3075 - val_acc: 0.9002\n",
      "Epoch 7/20\n",
      "8177/8177 [==============================] - 1s 113us/step - loss: 0.2328 - acc: 0.9305 - val_loss: 0.2963 - val_acc: 0.9037\n",
      "Epoch 8/20\n",
      "8177/8177 [==============================] - 1s 115us/step - loss: 0.2215 - acc: 0.9332 - val_loss: 0.2902 - val_acc: 0.9032\n",
      "Epoch 9/20\n",
      "8177/8177 [==============================] - 1s 110us/step - loss: 0.2101 - acc: 0.9364 - val_loss: 0.2869 - val_acc: 0.9046\n",
      "Epoch 10/20\n",
      "8177/8177 [==============================] - 1s 115us/step - loss: 0.2007 - acc: 0.9396 - val_loss: 0.2829 - val_acc: 0.9032\n",
      "Epoch 11/20\n",
      "8177/8177 [==============================] - 1s 130us/step - loss: 0.1937 - acc: 0.9400 - val_loss: 0.2800 - val_acc: 0.9042\n",
      "Epoch 12/20\n",
      "8177/8177 [==============================] - 1s 107us/step - loss: 0.1828 - acc: 0.9440 - val_loss: 0.2811 - val_acc: 0.9076\n",
      "Epoch 13/20\n",
      "8177/8177 [==============================] - 1s 113us/step - loss: 0.1781 - acc: 0.9470 - val_loss: 0.2786 - val_acc: 0.9086\n",
      "Epoch 14/20\n",
      "8177/8177 [==============================] - 1s 114us/step - loss: 0.1730 - acc: 0.9457 - val_loss: 0.2810 - val_acc: 0.9056\n",
      "Epoch 15/20\n",
      "8177/8177 [==============================] - 1s 115us/step - loss: 0.1679 - acc: 0.9466 - val_loss: 0.2775 - val_acc: 0.9042\n",
      "Epoch 16/20\n",
      "8177/8177 [==============================] - 1s 118us/step - loss: 0.1619 - acc: 0.9494 - val_loss: 0.2777 - val_acc: 0.9046\n",
      "Epoch 17/20\n",
      "8177/8177 [==============================] - 1s 113us/step - loss: 0.1583 - acc: 0.9508 - val_loss: 0.2749 - val_acc: 0.9051\n",
      "Epoch 18/20\n",
      "8177/8177 [==============================] - 1s 131us/step - loss: 0.1530 - acc: 0.9508 - val_loss: 0.2780 - val_acc: 0.9061\n",
      "Epoch 19/20\n",
      "8177/8177 [==============================] - 1s 114us/step - loss: 0.1500 - acc: 0.9537 - val_loss: 0.2749 - val_acc: 0.9081\n",
      "Epoch 20/20\n",
      "8177/8177 [==============================] - 1s 122us/step - loss: 0.1474 - acc: 0.9545 - val_loss: 0.2771 - val_acc: 0.9071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9b45d4eb8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Shape: (Batch Size, Feature Vector length)\n",
    "x = Input(shape=(X_train_gap.shape[1],))\n",
    "y = Dropout(0.2)(x)\n",
    "y = Dense(120, activation='softmax', kernel_initializer='he_normal', name='classifier')(y)\n",
    "model_gap = Model(inputs=x, outputs=y, name='GAP')\n",
    "model_gap.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "print('Trainable: %d, Non-Trainable: %d' % get_params_count(model_gap))\n",
    "\n",
    "# Prepare Callbacks for Model Checkpoint, Early Stopping and Tensorboard.\n",
    "log_name = '/DogBreed-EP{epoch:02d}-LOSS{val_loss:.4f}.h5'\n",
    "log_dir = datetime.now().strftime('gap_model_%Y%m%d_%H%M')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "mc = ModelCheckpoint(log_dir + log_name, monitor='val_loss', save_best_only=True)\n",
    "tb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model_gap.fit(x=X_train_gap, y=Y_train, batch_size=32, epochs=20, validation_data=(X_val_gap, Y_val), \n",
    "              callbacks=[es, mc, tb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
