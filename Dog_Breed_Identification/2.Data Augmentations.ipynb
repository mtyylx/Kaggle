{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像增强 Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from utils import get_params_count\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.applications import inception_v3, xception, resnet50, vgg16, vgg19\n",
    "from keras.applications import InceptionV3, Xception, ResNet50, VGG16, VGG19\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_input(height, width, input_type):\n",
    "    train_name = os.listdir('./train')\n",
    "    test_name = os.listdir('./test')\n",
    "    train_size = len(train_name)\n",
    "    test_size = len(test_name)\n",
    "    X_train = np.zeros((train_size, height, width, 3), dtype=input_type)\n",
    "    X_test = np.zeros((test_size, height, width, 3), dtype=input_type)\n",
    "    \n",
    "    labels = pd.read_csv('labels.csv')\n",
    "    breeds = list(set(labels['breed']))\n",
    "    breeds.sort()\n",
    "    Y_test = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "    # Labels\n",
    "    Y_train = np.zeros((train_size, len(breeds)), dtype=np.uint8)\n",
    "    for i in range(train_size):\n",
    "        onehot = breeds.index(labels['breed'][i])\n",
    "        Y_train[i][onehot] = 1\n",
    "\n",
    "    # Train data\n",
    "    for i in tqdm(range(train_size)):\n",
    "        img = cv2.imread('./train/%s.jpg' % labels['id'][i])\n",
    "        img = cv2.resize(img, dsize=(width, height))\n",
    "        img = img[:, :, ::-1]\n",
    "        X_train[i] = img\n",
    "\n",
    "    # Test data\n",
    "    for i in tqdm(range(test_size)):\n",
    "        img = cv2.imread('./test/%s.jpg' % Y_test['id'][i])\n",
    "        img = cv2.resize(img, dsize=(width, height))\n",
    "        img = img[:, :, ::-1]\n",
    "        X_test[i] = img\n",
    "\n",
    "    print(\"Train: %d, Test: %d\" % (train_size, test_size))\n",
    "    print(\"Total Dog Breeds:\", len(breeds))\n",
    "    print('Training Data Size = %.2f GB' % (sys.getsizeof(X_train)/1024**3))\n",
    "    print('Testing Data Size = %.2f GB' % (sys.getsizeof(X_test)/1024**3))\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 10222/10222 [00:54<00:00, 187.50it/s]\n",
      "100%|█████████████████████████████████████████████████████| 10357/10357 [00:55<00:00, 187.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10222, Test: 10357\n",
      "Total Dog Breeds: 120\n",
      "Training Data Size = 2.55 GB\n",
      "Testing Data Size = 2.59 GB\n"
     ]
    }
   ],
   "source": [
    "height = 299\n",
    "width = 299\n",
    "img_train, img_test, lb_train, lb_test = load_input(height, width, np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(img_train, lb_train, shuffle=True, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8177, 299, 299, 3), (2045, 299, 299, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Augment Training Data\n",
    "train_gen = ImageDataGenerator(\n",
    "    preprocessing_function=inception_v3.preprocess_input,\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    ")\n",
    "\n",
    "# Leave Validation Data intact\n",
    "test_gen = ImageDataGenerator(\n",
    "    preprocessing_function=inception_v3.preprocess_input,\n",
    ")\n",
    "\n",
    "# Fit generator\n",
    "train_gen.fit(X_train)\n",
    "test_gen.fit(X_val)\n",
    "\n",
    "# Patch X and Y together\n",
    "gen1 = train_gen.flow(X_train, y_train, 64)\n",
    "gen2 = test_gen.flow(X_val, y_val, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_model = InceptionV3(include_top=False, input_shape=(299, 299, 3), weights='imagenet', pooling='avg')\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "y = Dropout(0.2)(base_model.output)\n",
    "y = Dense(120, activation='softmax', kernel_initializer='he_normal')(y)\n",
    "model = Model(inputs=base_model.input, outputs=y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\tfgpu\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  if sys.path[0] == '':\n",
      "c:\\anaconda\\envs\\tfgpu\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(validation_steps=31, steps_per_epoch=127, generator=<keras.pre..., validation_data=<keras.pre..., callbacks=[<keras.ca..., epochs=1)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "127/127 [==============================] - 100s 791ms/step - loss: 0.8995 - acc: 0.7829 - val_loss: 0.6114 - val_acc: 0.8412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a29c097f98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = '.\\\\models\\\\' + datetime.now().strftime('transfer_model_%Y%m%d_%H%M')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "mc = ModelCheckpoint(log_dir + '\\\\DogBreed-EP{epoch:02d}-LOSS{val_loss:.4f}.h5', \n",
    "                     monitor='val_loss', save_best_only=True)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "model.fit_generator(generator=gen1, steps_per_epoch=8177//batch_size, nb_epoch=1, \n",
    "                    validation_data=gen2, validation_steps=2045//batch_size, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/1\n",
      "8177/8177 [==============================] - 57s 7ms/step - loss: 2.3378 - acc: 0.5540 - val_loss: 0.8132 - val_acc: 0.8059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a56350d978>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = '.\\\\models\\\\' + datetime.now().strftime('transfer_model_%Y%m%d_%H%M')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=20)\n",
    "mc = ModelCheckpoint(log_dir + '\\\\DogBreed-EP{epoch:02d}-LOSS{val_loss:.4f}.h5', \n",
    "                     monitor='val_loss', save_best_only=True)\n",
    "\n",
    "batch_size = 64\n",
    "model.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val), callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
